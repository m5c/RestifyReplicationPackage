{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"RESTify Experiment Replication Package","text":"<p>All you need to replicate our findings, reuse our data or tools.</p> <p>The simplest way to replicate our study findings, is the prepared docker image. Within just a few minutes you can power up a Jupyter Notebook and replicate all statistics and figures of our paper. You only need docker and a browser.</p>"},{"location":"index.html#about","title":"About","text":"<p>This webpage servers as entry point for the artifact submission of our MODELS 2024 conference contribution.</p> <ul> <li>Main purpose of this replication package is to allow fast and independent replication of all our results and interpretations.</li> <li>We carefully documented all our methodology, and automated our analysis. E.g. all paper figures are generated from raw data, and we provide you with the means to conveniently replicate them and validate correctness of our findings on your local machine.  </li> <li>Furthermore we publish all study material and tools used to conduct the study, so you can easily replicate the experiment, or reuse parts of our material for follow-up research.</li> </ul>"},{"location":"index.html#package-content","title":"Package Content","text":"<p>We now present the various components of our artifact submission. Each of items enumerated in the remainder corresponds to one entry on the navigation bar (left).</p>"},{"location":"index.html#replication","title":"Replication","text":"<ul> <li>Result Replication: Center-piece of our replication package is a Jupyter Notebook that takes you through the data analysis and replicates all paper figures on your machine.<ul> <li>We offer various levels of inspection, from static prerendered notebook, to dockerized local replication, to in-depth analysis of the implementation in PyCharm.</li> <li>For the replication we provide you with the raw data collected throughout the experiment. Then you crunch it locally and replicate the findings, all using convenient scripts.</li> <li>The data analysis was coded for easy inspection and transparency. You can at any point in time inspect the well-documented codenbase, and verify our implementation.</li> </ul> </li> </ul>"},{"location":"index.html#material","title":"Material","text":"<p>In addition to replication of our findings, we provide everything needed to replicate the experiment itself, i.e. we publish the sources for all material and tools involved the study conduct. We do so to provide transparency on all details of our methodology, and ease replication of our study by colleagues, as well as reuse of our tools for future related work.</p> <ul> <li>Recruitment Material: Sources and rendered HTML version of the webpage used for participant recruitment.</li> <li>Group Allocation Algo: Source code and documentation of the algorithm implemented to create a balanced participant groups (optimizes comparable group skillsets).</li> <li>Task Instruction Material: Sources and rendered HTML version with textual and video task instructions for all four control groups.</li> <li>Sample Legacy Applications (Objects): The sample applications used task for training or actual conversion task.</li> </ul>"},{"location":"index.html#data-and-evaluation","title":"Data and Evaluation","text":"<ul> <li>Submission Correctness Evaluation Tool: Sources and documentation of the analyzer tool that we implemented to assess correctness of participant submissions. A reusable tool to test REST APIs against a predefined interface and produce correctness reports.</li> <li>Raw Experiment Collected Data: Raw data collected throughout the experiment: CSV and textual summary files that conclude data extracted from video material viewing, such as transcript of observations and time measurements for the individual conversion tasks.</li> </ul>"},{"location":"about.html","title":"About","text":"<p>This is the replication package of the RESTify controlled Experiment. At the time of writing, the experiment reporting has been accepted as publication for the MODELS 2024 conference.</p> <ul> <li>First author / Principal investigator: Maximilian Schiedermeier</li> <li>Co authors / Academic supervisors: Bettina Kemme, J\u00f6rg Kienzle</li> </ul>"},{"location":"allocation.html","title":"Group Allocation","text":"<p>We implemented an algorithm to partition study participants on comparable groups. The algorithm is a heuristic that aims at minimizing skill differences between study groups. Although not needed to replicate our results, we release the implementation, for further transparency and easier extension of our work.</p>"},{"location":"allocation.html#source-code","title":"Source Code","text":"<p>The documented source code of the MiniMax implementation and documentation is available on GitHub.</p>"},{"location":"allocation.html#explanations","title":"Explanations","text":"<p>We implemented a custom MiniMax Heuristic, to assess the participant's skill profiles and create balanced experiment groups.  </p> <ul> <li>To maintain transparency we provide source code and documentation of the heuristic used to create the participant allocations.</li> <li>The program parses all provided self assessment forms (see corresponding entry of replication bundle) which they provided during recruitment, and then performs a MiniMax search for the fairest group allocation.</li> <li>A side product of this software is the generation of personalized emails for following communication with the individual participants. </li> </ul> <p>Since the purpose of this program is to select of all applicants and provide a mapping from their legal names to group-specific pseudonyms, it can be only executed with access to the sensitive participant sheets. The latter we do not release, to protect participant privacy.</p>"},{"location":"analyzer.html","title":"Submission Analyzer","text":"<p>A central component our study investigation was the assessment of code produced by study participants. Given the amount of code to analyze, we wrote an automated tool, to test the submissions against a stipulated target REST interface. Source code and documentation are available as open source on GitHub</p>"},{"location":"analyzer.html#how-it-works","title":"How it works","text":"<ul> <li>Download the raw participant code submissions.</li> <li>Clone the analyzer tool and follow the run instructions it. You will obtain a CSV test report with details on every participant submission.</li> <li>You can match the test reports against the data we used for our statistical analysis and paper figures. It serves as starting point for the result replication.</li> </ul>"},{"location":"data.html","title":"Collected Data","text":"<p>This page lists data we collected in preparation, or throughout the experiment. There are two limitations to consider:</p> <ul> <li>The data is anonymized, i.e. we wiped or replaced any information related to participant identity.</li> <li>This page contains no interpretation results, i.e. statistical insights, or conclusions. If you are interested in replicating our interpretation results (as presented in our paper submission), we kindly refer you to the dedicated result replication page.</li> </ul>"},{"location":"data.html#video-observations","title":"Video Observations","text":"<p>We collected more than 72 hours of video onscreen recordings throughout the experiment. Participants were asked to avoid capturing personal information or identifiers. Unfortunately this request was often ignored. To preserve participant anonymity, we do not provide original video material.</p> <p>However, we do provide a lot of information, carefully extracted from the video material:</p> <ul> <li>Task solving and task preparation times: We measured how much time every participant spent on the actual project conversion tasks and the instructions and provide precise information on their time spendings.</li> <li>Task deviations, difficulties, remarkable observations: We provide for each participant and each refactoring task a transcript of all noteworthy events. This includes problems with specific task phases, problems with software used, even information on their task solving activity itself.</li> <li>Participant methodology feedback preferences</li> </ul> <p>Below are direct links to the described data, in various file formats:</p> <ul> <li>Mac Numbers File</li> <li>Microsoft Excel File</li> <li>CSV File</li> </ul>"},{"location":"data.html#participant-feedback","title":"Participant Feedback","text":"<p>Participant provided a plethora of comments on the study, regarding issues, personal preferences, and general feedback. The comments were collected as final step of their study participation.</p> <p>Below are direct links to the described data, in various file formats:</p> <ul> <li>Mac Numbers File</li> <li>Microsoft Excel File</li> <li>CSV File </li> </ul> <p>We also created an informal meta summary of most common issues observed in task solving.</p>"},{"location":"data.html#code-model-submissions","title":"Code / Model Submissions","text":"<p>Each participant produced two software artifacts: One raw code submission, where the participant manually migrated an application to REST - and one model submission, where the migration was performed by help of modelling. Note that in the latter case the provided models were used to directly generate corresponding code.</p> <p>Below are direct links to download all participant software submissions.</p> <ul> <li>All raw code and all models (anonymized) (and the corresponding generated code), provided by participants.</li> <li>All patched code and all models (anonymized) (and the corresponding generated code), provided by participants.</li> </ul> <p>Patched refers to minimal changes done to participant submissions. In rare cases minimal project configuration mistakes prevented us from testing a submission. In these cases we minimally modified the participant submission. All study interpretations are based on the patched version. We describe the modifications applied in more detail in our paper submission.</p>"},{"location":"data.html#participant-test-results","title":"Participant Test Results","text":"<p>We determined correctness of all submissions, by testing against a stipulated REST API interface description. The process is described in more detail in the dedicated analyzer section.</p> <ul> <li>For your convenience, we provide a direct link with the test results in CSV format. </li> <li>You can reproduce the test results by following the instructions on the analyzer documentation page.</li> </ul>"},{"location":"material.html","title":"Training and Task Material","text":"<p>The original task training and task instructions material. One for each of the four experiment groups:</p> <ul> <li>Red Group</li> <li>Green Group</li> <li>Blue Group</li> <li>Yellow Group</li> </ul> <p>All websites were generated from a MkDocs markdown template. The sources are accessible, here.</p> <p>Task context and order changes, depending on which group a participant was allocated to.</p>"},{"location":"objects.html","title":"Objects","text":"<p>The study contains three sample legacy applications for migration to REST. One application (The Zoo) was used for illustrative purposes, the other two (The BookStore, Xox) served as study objects, i.e. participants were working with these two applications. We release the source code and build / run instructions for all three legacy java applications.</p>"},{"location":"objects.html#the-zoo","title":"The Zoo","text":"<p>The zoo is only used for subject training. It is smaller than the applications used for the migration tasks.</p> <ul> <li>Zoo Legacy Sources on GitHub</li> </ul>"},{"location":"objects.html#the-bookstore","title":"The BookStore","text":"<p>Legacy application No.1: A simple e-commerce application.</p> <ul> <li>BookStore Legacy Sources on GitHub</li> </ul>"},{"location":"objects.html#xox","title":"Xox","text":"<p>Legacy application No.2: A simple turn based board game classic.</p> <ul> <li>Xox Legacy Sources on GitHub</li> </ul>"},{"location":"recruitment.html","title":"Recruitment Webpage","text":"<p>Here you find anonymized references to the material used for recruitment:</p> <ul> <li>Markdown / MkDocs sources of our recruitment page.</li> <li>Navigable replica of our recruitment page.</li> <li>The auto assessment form used to assess preliminary participant skills.</li> </ul>"},{"location":"replication.html","title":"Result Replication","text":"<p>All statistical figures and listings in our paper submission were created programmatically. The code to produce them, from raw data (which we also provide) is publicly available and well documented.</p> <p>As part of the dedicated component, we provide:</p> <ul> <li>The raw data</li> <li>The code used for statistical analysis</li> <li>Instructions on how to rapidly run (and reuse!) the code</li> </ul>"},{"location":"replication.html#where-to-start","title":"Where to Start","text":"<p>Depending on how-thorough you want to dive into the result replication, here are your four options:</p> <ol> <li>Static inspection, rendered Jupyter Notebook: You can view a static, non-executable render of our Notebook on GitHub. You only need a browser.</li> <li>Dynamic result replication, using a docker configuration: You can execute and replicate all figures and results, with minimal installation effort. See install instructions for Docker</li> <li>Dynamic result replication, using your own Jupyter Notebook: You can replicate and extend our analysis. See install instructions for native Jupyter </li> <li>Dynamic result replication and function reuse, using the raw python sources, and an IDE: You have full control over all analysis functions and their internals. See install instructions for PyCharm </li> </ol>"}]}